{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22524001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00843e",
   "metadata": {},
   "source": [
    "## Data Loaders and Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec515a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\"\"\"\n",
    "pip install unstructured\n",
    "pip install \"unstructured[pdf]\"\n",
    "pip install \"unstructured[docx]\"\n",
    "\"\"\"\n",
    "chat = ChatOpenAI(\n",
    "                model=\"gpt-4.1-nano\",\n",
    "                temperature=0.1,\n",
    "                tiktoken_model_name=\"gpt-3.5-turbo\",\n",
    "                streaming = True,\n",
    "                callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./study_file.docx\")\n",
    "\n",
    "loader.load_and_split(text_splitter = splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704fbf5",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "\n",
    "vector = embedder.embed_documents([\n",
    "    \"hi\",\n",
    "    \"how\",\n",
    "    \"are\",\n",
    "    \"you? my name is Jay!\"\n",
    "    \n",
    "])\n",
    "\n",
    "len(vector[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35a55d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\"\"\"\n",
    "pip install chromadb\n",
    "pip install chroma --upgrade\n",
    "\"\"\"\n",
    "\n",
    "cache_dir = LocalFileStore(\"./cache/\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "                model=\"gpt-4.1-nano\",\n",
    "                temperature=0.1,\n",
    "                tiktoken_model_name=\"gpt-3.5-turbo\",\n",
    "                streaming = True,\n",
    "                callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./study_file.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter = splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "result = vectorstore.similarity_search(\"where does winston live\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66717db1",
   "metadata": {},
   "source": [
    "## RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae879e26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cache_dir = LocalFileStore(\"./cache/\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "#                 model=\"gpt-4.1-nano\",\n",
    "                temperature=0.1,\n",
    "                tiktoken_model_name=\"gpt-3.5-turbo\",\n",
    "                streaming = True,\n",
    "                callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./study_file.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter = splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    chain_type = \"stuff\",  # stuff, refine,map_reduce, map_rerank\n",
    "    retriever = vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "# chain.run(\"where does winston live\")\n",
    "chain.run(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570d75f",
   "metadata": {},
   "source": [
    "## Stuff LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cache_dir = LocalFileStore(\"./cache/\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "#                 model=\"gpt-4.1-nano\",\n",
    "                temperature=0.1,\n",
    "                tiktoken_model_name=\"gpt-3.5-turbo\",\n",
    "                streaming = True,\n",
    "                callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./study_file.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter = splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\":retriver, \"question\":RunnablePassthrough()} | prompt | chat\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380cb094",
   "metadata": {},
   "source": [
    "## Map Reduce LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48d845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cache_dir = LocalFileStore(\"./cache/\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "#                 model=\"gpt-4.1-nano\",\n",
    "                temperature=0.1,\n",
    "                tiktoken_model_name=\"gpt-3.5-turbo\",\n",
    "                streaming = True,\n",
    "                callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./study_file.docx\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter = splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | chat\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    results = []\n",
    "    for document in documents:\n",
    "        result = map_doc_chain.invoke({\n",
    "            \"context\":document.page_content,\n",
    "            \"question\":question,\n",
    "            \n",
    "        }).content\n",
    "        results.append(result)\n",
    "    results = \"\\n\\n\".join(results)\n",
    "    return results\n",
    "\n",
    "map_chain = {\"documents\":retriever, \"question\":RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain,\"question\":RunnablePassthrough()} |final_prompt | chat\n",
    "\n",
    "chain.invoke(\"where does Winston go to work?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc9c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
